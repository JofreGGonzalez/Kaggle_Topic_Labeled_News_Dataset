{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bdd80b8",
   "metadata": {},
   "source": [
    "### Cas Kaggke: Topic Labeled News Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90104c1",
   "metadata": {},
   "source": [
    "En aquest document veurem, analitzarem i compararem diferents classificadors sobre una base de dades de Kaggle. <br/><br/>\n",
    "Els objectius principals son:\n",
    "- Analitzar la base de dades\n",
    "- Aplicar diferents classificadors \n",
    "- Analitzar els resultats obtinguts dels classificadors\n",
    "- Extreure conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e3218a",
   "metadata": {},
   "source": [
    "La base de dades de kaggle utilitzada és la de \"Topic Labeled News Dataset\". La base de dades és una recopilació d'articles de notícies. A continucació començarem veient i analitzant la base de dades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "015672e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importem llibreries\n",
    "import ipywidgets as widgets\n",
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib notebook\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np #importem la llibreria\n",
    "import random\n",
    "\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c3ee358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcio per a llegir dades en format csv\n",
    "def load_dataset(path):\n",
    "    dataset = pd.read_csv(path,sep = None,engine='python')\n",
    "    return dataset\n",
    "\n",
    "# Carreguem dataset asignat\n",
    "dataset = load_dataset('labelled_newscatcher_dataset.csv')\n",
    "data = dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9651a4c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>link</th>\n",
       "      <th>domain</th>\n",
       "      <th>published_date</th>\n",
       "      <th>title</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>https://www.eurekalert.org/pub_releases/2020-0...</td>\n",
       "      <td>eurekalert.org</td>\n",
       "      <td>2020-08-06 13:59:45</td>\n",
       "      <td>A closer look at water-splitting's solar fuel ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>https://www.pulse.ng/news/world/an-irresistibl...</td>\n",
       "      <td>pulse.ng</td>\n",
       "      <td>2020-08-12 15:14:19</td>\n",
       "      <td>An irresistible scent makes locusts swarm, stu...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>https://www.express.co.uk/news/science/1322607...</td>\n",
       "      <td>express.co.uk</td>\n",
       "      <td>2020-08-13 21:01:00</td>\n",
       "      <td>Artificial intelligence warning: AI will know ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>https://www.ndtv.com/world-news/glaciers-could...</td>\n",
       "      <td>ndtv.com</td>\n",
       "      <td>2020-08-03 22:18:26</td>\n",
       "      <td>Glaciers Could Have Sculpted Mars Valleys: Study</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>https://www.thesun.ie/tech/5742187/perseid-met...</td>\n",
       "      <td>thesun.ie</td>\n",
       "      <td>2020-08-12 19:54:36</td>\n",
       "      <td>Perseid meteor shower 2020: What time and how ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     topic                                               link          domain  \\\n",
       "0  SCIENCE  https://www.eurekalert.org/pub_releases/2020-0...  eurekalert.org   \n",
       "1  SCIENCE  https://www.pulse.ng/news/world/an-irresistibl...        pulse.ng   \n",
       "2  SCIENCE  https://www.express.co.uk/news/science/1322607...   express.co.uk   \n",
       "3  SCIENCE  https://www.ndtv.com/world-news/glaciers-could...        ndtv.com   \n",
       "4  SCIENCE  https://www.thesun.ie/tech/5742187/perseid-met...       thesun.ie   \n",
       "\n",
       "        published_date                                              title lang  \n",
       "0  2020-08-06 13:59:45  A closer look at water-splitting's solar fuel ...   en  \n",
       "1  2020-08-12 15:14:19  An irresistible scent makes locusts swarm, stu...   en  \n",
       "2  2020-08-13 21:01:00  Artificial intelligence warning: AI will know ...   en  \n",
       "3  2020-08-03 22:18:26   Glaciers Could Have Sculpted Mars Valleys: Study   en  \n",
       "4  2020-08-12 19:54:36  Perseid meteor shower 2020: What time and how ...   en  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42834548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>link</th>\n",
       "      <th>domain</th>\n",
       "      <th>published_date</th>\n",
       "      <th>title</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>108774</td>\n",
       "      <td>108774</td>\n",
       "      <td>108774</td>\n",
       "      <td>108774</td>\n",
       "      <td>108774</td>\n",
       "      <td>108774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>8</td>\n",
       "      <td>106130</td>\n",
       "      <td>5164</td>\n",
       "      <td>68743</td>\n",
       "      <td>103180</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>https://www.google.com/</td>\n",
       "      <td>dailymail.co.uk</td>\n",
       "      <td>2020-08-04 01:00:00</td>\n",
       "      <td>US tops 5 million confirmed virus cases, to Eu...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>15000</td>\n",
       "      <td>19</td>\n",
       "      <td>1855</td>\n",
       "      <td>41</td>\n",
       "      <td>21</td>\n",
       "      <td>108774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                topic                     link           domain  \\\n",
       "count          108774                   108774           108774   \n",
       "unique              8                   106130             5164   \n",
       "top     ENTERTAINMENT  https://www.google.com/  dailymail.co.uk   \n",
       "freq            15000                       19             1855   \n",
       "\n",
       "             published_date  \\\n",
       "count                108774   \n",
       "unique                68743   \n",
       "top     2020-08-04 01:00:00   \n",
       "freq                     41   \n",
       "\n",
       "                                                    title    lang  \n",
       "count                                              108774  108774  \n",
       "unique                                             103180       1  \n",
       "top     US tops 5 million confirmed virus cases, to Eu...      en  \n",
       "freq                                                   21  108774  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6261446e",
   "metadata": {},
   "source": [
    "Com es pot observar, la BD està formada per 6 atributs:\n",
    "- **topic:** topic en el que s'ha classificat l'article, segons el Kaggle n'hi han 8 tipos(BUSINESS, ENTERTAINMENT, HEALTH, NATION, SCIENCE, SPORTS, TECHNOLOGY i WORLD)\n",
    "- **link:** link d'on trobar l'article\n",
    "- **domain:** domini de la pàgina en que va ser publicat l'article\n",
    "- **published_date:** data en que l'article va ser publicat\n",
    "- **title:** títol de l'article\n",
    "- **lang:** llengua en el que està escrit l'article, podem observar com tots els articles estàn en Anglés\n",
    "\n",
    "Ara continuem analitzant la base de dades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a945e614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic             object\n",
      "link              object\n",
      "domain            object\n",
      "published_date    object\n",
      "title             object\n",
      "lang              object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dataset.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dcb0a48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic             0\n",
      "link              0\n",
      "domain            0\n",
      "published_date    0\n",
      "title             0\n",
      "lang              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3728fe55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-09-16 04:44:50\n",
      "2020-08-18 05:49:00\n"
     ]
    }
   ],
   "source": [
    "print (dataset['published_date'].min())\n",
    "print (dataset['published_date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c11ba224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ENTERTAINMENT    15000\n",
       "HEALTH           15000\n",
       "SPORTS           15000\n",
       "TECHNOLOGY       15000\n",
       "WORLD            15000\n",
       "NATION           15000\n",
       "BUSINESS         15000\n",
       "SCIENCE           3774\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contTopic = dataset['topic'].value_counts()\n",
    "contTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308299a6",
   "metadata": {},
   "source": [
    "Observacions importants:\n",
    "- no tenim nungún atribut numéric\n",
    "- no tenim ninguna entrada / fila en la base de dades amb algún valor a null\n",
    "- podem observar com hi han 8 topics diferents, 7 d'ells consten de 15000 entrades en la BD i un d'ells (el de SCIENCE) 3774\n",
    "- la data de publicació dels articles més antiga és del 2012-09-16 i la més recent del 2020-08-18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b55711",
   "metadata": {},
   "source": [
    "He decidit entrenar i comparar classificadors que a partir del títol (camp \"title\" en la BD) faci una classificació d'un article segons el topic (camp objectiu \"topic\").\n",
    "Descarto la llengua ja que tots estàn escrits en la mateixa llengua, el link, el domini i la data de publicació perque no aportarien ninguna informació sobre el tópic de l'article. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214ec073",
   "metadata": {},
   "source": [
    "A continuació, entrenarem un classificador Naive Bayes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de3d32d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct classification Naive Bayes with stopwords       0.1 % of the data:  0.7868174296745726\n",
      "Correct classification Naive Bayes with stopwords       0.2 % of the data:  0.7875890599862101\n",
      "Correct classification Naive Bayes with stopwords       0.4 % of the data:  0.7785796368650885\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X = dataset['title']\n",
    "Y = dataset['topic'] #objectiu\n",
    "\n",
    "particions = [0.1, 0.2, 0.4] #porcentatge de dades que s'utilitzaran en test\n",
    "\n",
    "for part in particions:\n",
    "    # Dividim dades en part d'entrenament i test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=part, random_state=0)\n",
    "\n",
    "    #Creem un pipeline, que serveix per aplicar seqüencialment una llista de transformades i un \n",
    "    #estimador final (l'estimador final ha de portar implementat \"fit\").\n",
    "    #\n",
    "    #La funció de \"CountVectorizer\" s'utilitza per convertir un text en un vector de recomptes, es a dir, \n",
    "    #retorna un diccionari de paraules i el seu recompte de vagades que ha sortit cada paraula.\n",
    "    #\n",
    "    #La funció de \"TfidfTransformer\" transforma el diccionari / recompte de la funció \"CountVectorizer\"\n",
    "    #en una representació normalitzada tf (frequéncia) o tf-idf (frequéncia multiplicada per la inversa de la frequéncia),\n",
    "    #aixó fa que obtinguem la frequéncia d'aparició d'una paraula.\n",
    "    #\n",
    "    #Finalment, tenim la funció \"MultinomialNB\" que és el classificador multinomial Naive Bayes, s'utilitza aquest perque \n",
    "    #és adequat per a la classificació amb característiques discretes (p. ex., recompte de paraules per a la classificació \n",
    "    #de text)\n",
    "    \n",
    "    naiveBayes = Pipeline([('countVect', CountVectorizer()),\n",
    "                          ('tfidfTransf', TfidfTransformer()),\n",
    "                          ('clf', MultinomialNB()),\n",
    "    ])\n",
    "    \n",
    "    #entrenem\n",
    "    naiveBayes = naiveBayes.fit(x_train, y_train)\n",
    "\n",
    "    #test\n",
    "    predicted = naiveBayes.predict(x_test)\n",
    "    print (\"Correct classification Naive Bayes with stopwords      \", part, \"% of the data: \", np.mean(predicted == y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37299c7f",
   "metadata": {},
   "source": [
    "Com podem observar, el millor accuracy que s'ha obtingut ha sigut l'entrenat amb el 80% de les dades amb un accuracy del 78,75%.  <br/><br/>\n",
    "Cal mencionar que tots els títols dels articles, podien contenir paraules que no aportaven cap informació útil per decidir en quina categoria s'havia de classificar un text (\"stopwowrds\") com per exemple articles, preposicions, etc...\n",
    "Així que per comprovar si aquestes paraules tenien alguna influencia alhora de classificar els articles, les eliminarem i tornarem a entrenar el model. Per a fer-ho s'utilitza el métode \"remove_stopwords\" de la llibreria \"gensim.parsing.preprocessing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49b9ef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminem els stopwords\n",
    "dataTitleSW = dataset['title'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1da3f774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    A closer look water-splitting's solar fuel pot...\n",
       "1    An irresistible scent makes locusts swarm, stu...\n",
       "2    Artificial intelligence warning: AI know bette...\n",
       "3     Glaciers Could Have Sculpted Mars Valleys: Study\n",
       "4    Perseid meteor shower 2020: What time huge bri...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mirem si ha cannviat\n",
    "dataTitleSW.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9820580b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct classification Naive Bayes withought stopwords       0.1 % of the data:  0.793252436109579\n",
      "Correct classification Naive Bayes withought stopwords       0.2 % of the data:  0.7942541944380602\n",
      "Correct classification Naive Bayes withought stopwords       0.4 % of the data:  0.7823948517582165\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X = dataTitleSW\n",
    "Y = dataset['topic'] #objectiu\n",
    "\n",
    "particions = [0.1, 0.2, 0.4] #porcentatge de dades que s'utilitzaran en test\n",
    "\n",
    "for part in particions:\n",
    "    # Dividim dades en part d'entrenament i test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=part, random_state=0)\n",
    "\n",
    "    #Creem un pipeline, que serveix per aplicar seqüencialment una llista de transformades i un \n",
    "    #estimador final (l'estimador final ha de portar implementat \"fit\").\n",
    "    #\n",
    "    #La funció de \"CountVectorizer\" s'utilitza per convertir un text en un vector de recomptes, es a dir, \n",
    "    #retorna un diccionari de paraules i el seu recompte de vagades que ha sortit cada paraula.\n",
    "    #\n",
    "    #La funció de \"TfidfTransformer\" transforma el diccionari / recompte de la funció \"CountVectorizer\"\n",
    "    #en una representació normalitzada tf (frequéncia) o tf-idf (frequéncia multiplicada per la inversa de la frequéncia),\n",
    "    #aixó fa que obtinguem la frequéncia d'aparició d'una paraula.\n",
    "    #\n",
    "    #Finalment, tenim la funció \"MultinomialNB\" que és el classificador multinomial Naive Bayes, s'utilitza aquest perque \n",
    "    #és adequat per a la classificació amb característiques discretes (p. ex., recompte de paraules per a la classificació \n",
    "    #de text)\n",
    "    \n",
    "    naiveBayes = Pipeline([('countVect', CountVectorizer()),\n",
    "                          ('tfidfTransf', TfidfTransformer()),\n",
    "                          ('clf', MultinomialNB()),\n",
    "    ])\n",
    "    \n",
    "    #entrenem\n",
    "    naiveBayes = text_clf.fit(x_train, y_train)\n",
    "\n",
    "    #test\n",
    "    predicted = naiveBayes.predict(x_test)\n",
    "    print (\"Correct classification Naive Bayes withought stopwords      \", part, \"% of the data: \", np.mean(predicted == y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a91bb0",
   "metadata": {},
   "source": [
    "Com s'observa, un cop eliminats els \"stopwords\" notem un molt petit increment en el porcentatge d'acerts que té el classificador, passant d'un 78,75% a un 79,42%. Aquesta millora és mínima i apenes notoria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd8ca9",
   "metadata": {},
   "source": [
    "Ara anem a entrenar un classificador SVM (Support Vector Machines ) i el compararem els resultats amb el de Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1f15a845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct classification SVM with stopwords       0.1 % of the data:  0.7297297297297297\n",
      "Correct classification SVM with stopwords       0.2 % of the data:  0.7306366352562629\n",
      "Correct classification SVM with stopwords       0.4 % of the data:  0.728430245920478\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "X = dataset['title']\n",
    "Y = dataset['topic'] #objectiu\n",
    "\n",
    "particions = [0.1, 0.2, 0.4] #porcentatge de dades que s'utilitzaran en test\n",
    "\n",
    "for part in particions:\n",
    "    # Dividim dades en part d'entrenament i test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=part, random_state=0)\n",
    "\n",
    "    #Creem un pipeline, que serveix per aplicar seqüencialment una llista de transformades i un \n",
    "    #estimador final (l'estimador final ha de portar implementat \"fit\").\n",
    "    #\n",
    "    #La funció de \"CountVectorizer\" s'utilitza per convertir un text en un vector de recomptes, es a dir, \n",
    "    #retorna un diccionari de paraules i el seu recompte de vagades que ha sortit cada paraula.\n",
    "    #\n",
    "    #La funció de \"TfidfTransformer\" transforma el diccionari / recompte de la funció \"CountVectorizer\"\n",
    "    #en una representació normalitzada tf (frequéncia) o tf-idf (frequéncia multiplicada per la inversa de la frequéncia),\n",
    "    #aixó fa que obtinguem la frequéncia d'aparició d'una paraula.\n",
    "    #\n",
    "    #Finalment, tenim la funció \"MultinomialNB\" que és el classificador multinomial Naive Bayes, s'utilitza aquest perque \n",
    "    #és adequat per a la classificació amb característiques discretes (p. ex., recompte de paraules per a la classificació \n",
    "    #de text)\n",
    "    \n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('svc', svm.SVC(C=0.001, kernel='linear', gamma=0.9, probability=True)),\n",
    "    ])\n",
    "    \n",
    "    #entrenem\n",
    "    svmMulti.fit(x_train, y_train)\n",
    "    \n",
    "    #test\n",
    "    predicted_svm = svmMulti.predict(x_test)\n",
    "    \n",
    "    print (\"Correct classification SVM with stopwords      \", part, \"% of the data: \", np.mean(predicted_svm == y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "102e8626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct classification SVM withought stopwords       0.1 % of the data:  0.7336826622540908\n",
      "Correct classification SVM withought stopwords       0.2 % of the data:  0.7331188232590209\n",
      "Correct classification SVM withought stopwords       0.4 % of the data:  0.7315789473684211\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "X = dataTitleSW\n",
    "Y = dataset['topic'] #objectiu\n",
    "\n",
    "particions = [0.1, 0.2, 0.4] #porcentatge de dades que s'utilitzaran en test\n",
    "\n",
    "for part in particions:\n",
    "    # Dividim dades en part d'entrenament i test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=part, random_state=0)\n",
    "\n",
    "    #Creem un pipeline, que serveix per aplicar seqüencialment una llista de transformades i un \n",
    "    #estimador final (l'estimador final ha de portar implementat \"fit\").\n",
    "    #\n",
    "    #La funció de \"CountVectorizer\" s'utilitza per convertir un text en un vector de recomptes, es a dir, \n",
    "    #retorna un diccionari de paraules i el seu recompte de vagades que ha sortit cada paraula.\n",
    "    #\n",
    "    #La funció de \"TfidfTransformer\" transforma el diccionari / recompte de la funció \"CountVectorizer\"\n",
    "    #en una representació normalitzada tf (frequéncia) o tf-idf (frequéncia multiplicada per la inversa de la frequéncia),\n",
    "    #aixó fa que obtinguem la frequéncia d'aparició d'una paraula.\n",
    "    #\n",
    "    #Finalment, tenim la funció \"MultinomialNB\" que és el classificador multinomial Naive Bayes, s'utilitza aquest perque \n",
    "    #és adequat per a la classificació amb característiques discretes (p. ex., recompte de paraules per a la classificació \n",
    "    #de text)\n",
    "    \n",
    "    #svmMulti = Pipeline([('vect', CountVectorizer()),\n",
    "     #                     ('tfidf', TfidfTransformer()),\n",
    "     #                     ('clf-svm', SGDClassifier(alpha=0.01)),\n",
    "     #])\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('svc', svm.SVC(C=0.001, kernel='linear', gamma=0.9, probability=True)),\n",
    "    ])\n",
    "\n",
    "    #entrenem\n",
    "    svmMulti.fit(x_train, y_train)\n",
    "    \n",
    "    #test\n",
    "    predicted_svm = svmMulti.predict(x_test)\n",
    "    \n",
    "    print (\"Correct classification SVM withought stopwords      \", part, \"% of the data: \", np.mean(predicted_svm == y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d872f3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d140131",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print (\"Correct classification SVM: \", svc.score(x_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('svc', svm.SVC(C=0.01, kernel='linear', gamma=0.9, probability=True)),\n",
    "])\n",
    "text_clf = text_clf.fit(x_train, y_train)\n",
    "\n",
    "predicted = text_clf.predict(x_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd70ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
